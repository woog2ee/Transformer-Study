{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### set device and data"
      ],
      "metadata": {
        "id": "M9D6XKhI0bhP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPxJ-b-o0TTX",
        "outputId": "0e39b6d7-c7d2-4165-8121-c7a805d5649d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'using device: {DEVICE}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def file2list(file):\n",
        "    lst = []\n",
        "    for line in open(f'./{file}', 'r', encoding='utf-8-sig'):\n",
        "      line = line.replace('\\n', '')\n",
        "      lst.append(line)\n",
        "    return lst\n",
        "\n",
        "# train, valid, test texts\n",
        "en_train_lst = file2list('train.en')\n",
        "en_valid_lst = file2list('val.en')\n",
        "en_test_lst  = file2list('test.en')\n",
        "print(f'en data: {len(en_train_lst), len(en_valid_lst), len(en_test_lst)}')\n",
        "\n",
        "de_train_lst = file2list('train.de')\n",
        "de_valid_lst = file2list('val.de')\n",
        "de_test_lst  = file2list('test.de')\n",
        "print(f'de data: {len(de_train_lst), len(de_valid_lst), len(de_test_lst)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxz9V-pS0epR",
        "outputId": "2d6c6798-897c-4857-ccd4-8eb53a56b69d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en data: (29001, 1015, 1000)\n",
            "de data: (29001, 1015, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build vocabularies"
      ],
      "metadata": {
        "id": "yCG73n_60h4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy tokenizer\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbSOmekC0hR4",
        "outputId": "24ccdb26-a245-43b1-fb0c-f4aeca527dfa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# using spacy tokenizer\n",
        "en_tokenizer = get_tokenizer(tokenizer='spacy', language='en_core_web_sm')\n",
        "de_tokenizer = get_tokenizer(tokenizer='spacy', language='de_core_news_sm')\n",
        "\n",
        "# vocabulary for training texts\n",
        "en_vocab = build_vocab_from_iterator(map(en_tokenizer, [text for text in en_train_lst]),\n",
        "                                     min_freq=2,\n",
        "                                     specials=['<unk>','<sos>','<eos>','<pad>'],\n",
        "                                     special_first=True)\n",
        "de_vocab = build_vocab_from_iterator(map(de_tokenizer, [text for text in de_train_lst]),\n",
        "                                     min_freq=2,\n",
        "                                     specials=['<unk>','<sos>','<eos>','<pad>'],\n",
        "                                     special_first=True)\n",
        "print(f'en vocab size: {len(en_vocab)}')\n",
        "print(f'de vocab size: {len(de_vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXDbvvaG0oIC",
        "outputId": "6744873a-5a03-4414-cbdc-61717a19b030"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en vocab size: 6191\n",
            "de vocab size: 8014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make preprocessor for encode & decode texts"
      ],
      "metadata": {
        "id": "gKS7_Pmz0r3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor:\n",
        "  unk_token_id = 0\n",
        "  sos_token_id = 1\n",
        "  eos_token_id = 2\n",
        "  pad_token_id = 3\n",
        "\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    # token -> id\n",
        "    self.src_token2id = src_vocab.get_stoi()\n",
        "    self.tgt_token2id = tgt_vocab.get_stoi()\n",
        "    # id -> token\n",
        "    self.src_id2token = src_vocab.get_itos()\n",
        "    self.tgt_id2token = tgt_vocab.get_itos()\n",
        "\n",
        "  # encode token -> id for source sentence\n",
        "  def src_encode(self, text):\n",
        "    if type(text) == list: text = ' '.join(text)\n",
        "    tokenized = self.src_tokenizer(text)\n",
        "    encoded   = [self.src_token2id.get(token, self.src_token2id['<unk>']) for token in tokenized]\n",
        "    return encoded\n",
        "\n",
        "  # encode token -> id, attach <sos> and <eos> for target sentence\n",
        "  def tgt_encode(self, text):\n",
        "    if type(text) == list: text = ' '.join(text)\n",
        "    tokenized = self.tgt_tokenizer(text)\n",
        "    encoded   = [self.tgt_token2id['<sos>']] \\\n",
        "    + [self.tgt_token2id.get(token, self.tgt_token2id['<unk>']) for token in tokenized] \\\n",
        "    + [self.tgt_token2id['<eos>']]\n",
        "    return encoded\n",
        "\n",
        "  # decode source sentence\n",
        "  def src_decode(self, ids):\n",
        "    decoded = list(map(lambda x: self.src_id2token[x], ids))\n",
        "    return ' '.join(decoded)\n",
        "\n",
        "  # decode target sentence\n",
        "  def tgt_decode(self, ids):\n",
        "    decoded = list(map(lambda x: self.tgt_id2token[x], ids))[1:-1]\n",
        "    return ' '.join(decoded)\n",
        "\n",
        "preprocessor = Preprocessor(en_tokenizer, de_tokenizer, en_vocab, de_vocab)"
      ],
      "metadata": {
        "id": "U7DtZBqJ0uET"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make custom dataset, data loader"
      ],
      "metadata": {
        "id": "KP945MF50xzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, en_lst, de_lst, preprocessor):\n",
        "    self.en_lst = en_lst\n",
        "    self.de_lst = de_lst\n",
        "    assert len(self.en_lst) == len(self.de_lst)\n",
        "\n",
        "    self.preprocessor = preprocessor\n",
        "    self.dataset      = self.make_dataset()\n",
        "\n",
        "  # make (en, de) sentence pair dataset\n",
        "  def make_dataset(self):\n",
        "    dataset = [(self.preprocessor.src_encode(en), self.preprocessor.tgt_encode(de))\n",
        "               for en, de in zip(self.en_lst, self.de_lst)\n",
        "               if len(en) > 0 and len(de) > 0]\n",
        "    return dataset\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.dataset[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "train_dataset = CustomDataset(en_train_lst, de_train_lst, preprocessor)\n",
        "valid_dataset = CustomDataset(en_valid_lst, de_valid_lst, preprocessor)\n",
        "test_dataset  = CustomDataset(en_test_lst, de_test_lst, preprocessor)\n",
        "print(f'dataset size: {len(train_dataset), len(valid_dataset), len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz9qWRLS0xAf",
        "outputId": "16fea66d-bb7c-48a1-a9ed-866913b1f331"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset size: (29000, 1014, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# sequence padding in batch\n",
        "def collate_fn(batch_samples):\n",
        "  pad_token_id = preprocessor.pad_token_id\n",
        "  src_sent = pad_sequence([torch.tensor(src) for src, _ in batch_samples],\n",
        "                          batch_first=True,\n",
        "                          padding_value=pad_token_id)\n",
        "  tgt_sent = pad_sequence([torch.tensor(tgt) for _, tgt in batch_samples],\n",
        "                          batch_first=True,\n",
        "                          padding_value=pad_token_id)\n",
        "  return src_sent.to(DEVICE), tgt_sent.to(DEVICE)\n",
        "\n",
        "# composing batch to match similar lengths of sequences\n",
        "def batch_sampling(sequence_lengths, BATCH_SIZE):\n",
        "  seq_lens = [(i, seq_len, tgt_len) for i, (seq_len, tgt_len) in enumerate(sequence_lengths)]\n",
        "  seq_lens = sorted(seq_lens, key=lambda x: x[1])\n",
        "  seq_lens = [sample[0] for sample in seq_lens]\n",
        "\n",
        "  sample_indices = [seq_lens[i:i+BATCH_SIZE] for i in range(0, len(seq_lens), BATCH_SIZE)]\n",
        "  random.shuffle(sample_indices)\n",
        "  return sample_indices\n",
        "\n",
        "# make dataloader using sequence padding & composing batch\n",
        "def make_loader(dataset, BATCH_SIZE):\n",
        "  sequence_lengths = list(map(lambda x: (len(x[0]), len(x[1])), dataset))\n",
        "  batch_sampler = batch_sampling(sequence_lengths, BATCH_SIZE)\n",
        "  loader = DataLoader(dataset,\n",
        "                      collate_fn=collate_fn,\n",
        "                      batch_sampler=batch_sampler)\n",
        "  return loader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = make_loader(train_dataset, BATCH_SIZE)\n",
        "valid_loader = make_loader(valid_dataset, BATCH_SIZE)\n",
        "test_loader  = make_loader(test_dataset, BATCH_SIZE)\n",
        "print('check dataset and batch size,')\n",
        "\n",
        "print(f'train data length       : {len(en_train_lst)}')\n",
        "print(f'train data loader length: {len(train_loader)}')\n",
        "print(f'train data loader length: {len(en_train_lst) / BATCH_SIZE}\\n')\n",
        "\n",
        "print(f'valid data length       : {len(en_valid_lst)}')\n",
        "print(f'valid data loader length: {len(valid_loader)}')\n",
        "print(f'valid data loader length: {len(en_valid_lst) / BATCH_SIZE}\\n')\n",
        "\n",
        "print(f'test data length       : {len(en_test_lst)}')\n",
        "print(f'test data loader length: {len(test_loader)}')\n",
        "print(f'test data loader length: {len(en_test_lst) / BATCH_SIZE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNLK8Z1w01JE",
        "outputId": "6cf7b839-cd1c-4d08-e92a-ed4826c24fa1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check dataset and batch size,\n",
            "train data length       : 29001\n",
            "train data loader length: 454\n",
            "train data loader length: 453.140625\n",
            "\n",
            "valid data length       : 1015\n",
            "valid data loader length: 16\n",
            "valid data loader length: 15.859375\n",
            "\n",
            "test data length       : 1000\n",
            "test data loader length: 16\n",
            "test data loader length: 15.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### token embedding & positional encoding"
      ],
      "metadata": {
        "id": "acFO3ABL1JX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self, VOCAB_SIZE, HIDDEN_DIM):\n",
        "    super().__init__()\n",
        "    self.HIDDEN_DIM = HIDDEN_DIM\n",
        "    self.embedding  = nn.Embedding(VOCAB_SIZE, HIDDEN_DIM)\n",
        "    self.scale      = torch.sqrt(torch.FloatTensor([HIDDEN_DIM])).to(DEVICE)\n",
        "\n",
        "  def forward(self, src):\n",
        "    return self.embedding(src) * self.scale"
      ],
      "metadata": {
        "id": "irX6m31X04dF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, HIDDEN_DIM, dropout_ratio=0.1, max_length=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    pos = torch.arange(max_length).unsqueeze(1)\n",
        "    den = torch.exp(torch.arange(0, HIDDEN_DIM, 2) * (-math.log(10000) / HIDDEN_DIM))\n",
        "    \n",
        "    pos_embedding = torch.zeros(max_length, 1, HIDDEN_DIM)\n",
        "    pos_embedding[:, 0, 0::2] = torch.sin(pos * den)\n",
        "    pos_embedding[:, 0, 1::2] = torch.cos(pos * den)\n",
        "    self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "  def forward(self, token_embedding):\n",
        "    # x: [seq_len, batch_size, hidden_dim]\n",
        "    token_embedding += self.pos_embedding[:token_embedding.size(0), :]\n",
        "    return self.dropout(token_embedding)"
      ],
      "metadata": {
        "id": "vfgHFu5n2dA4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### masking"
      ],
      "metadata": {
        "id": "xWIuxI8M2fEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "  mask = (torch.triu(torch.ones((size, size), device=DEVICE)) == 1).transpose(0, 1)\n",
        "  mask = mask.float().masked_fill(mask==0, float('-inf'))\\\n",
        "                     .masked_fill(mask==1, float(0.0))\n",
        "  return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[0]\n",
        "  src_mask    = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "  \n",
        "  tgt_seq_len = tgt.shape[0]\n",
        "  tgt_mask    = generate_square_subsequent_mask(tgt_seq_len)\n",
        "\n",
        "  pad_idx = 3  # Preprocessor's pad_token_id\n",
        "  src_padding_mask = (src == pad_idx).transpose(0, 1)\n",
        "  tgt_padding_mask = (tgt == pad_idx).transpose(0, 1)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "RfpASFnb2ejy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer"
      ],
      "metadata": {
        "id": "xFpu3Gyt2heS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Transformer\n",
        "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
        "\n",
        "class Transformer_(nn.Module):\n",
        "  def __init__(self, INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, LAYERS, HEADS, PF_DIM, dropout_ratio):\n",
        "    super().__init__()\n",
        "    self.LAYERS = LAYERS\n",
        "    self.src_token_embedding = TokenEmbedding(INPUT_DIM, HIDDEN_DIM)\n",
        "    self.tgt_token_embedding = TokenEmbedding(OUTPUT_DIM, HIDDEN_DIM)\n",
        "    self.positional_encoding = PositionalEncoding(HIDDEN_DIM, dropout_ratio)\n",
        "  \n",
        "    self.transformer_encoders = nn.ModuleList([\n",
        "                                          TransformerEncoderLayer(HIDDEN_DIM, HEADS, PF_DIM, dropout_ratio)\n",
        "                                          for _ in range(LAYERS)])\n",
        "    self.transformer_decoders = nn.ModuleList([\n",
        "                                          TransformerDecoderLayer(HIDDEN_DIM, HEADS, PF_DIM, dropout_ratio)\n",
        "                                          for _ in range(LAYERS)])\n",
        "    self.fc_out = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask,\n",
        "              src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "    src_emb = self.src_token_embedding(src)\n",
        "    tgt_emb = self.tgt_token_embedding(tgt)\n",
        "    \n",
        "    src_emb = self.positional_encoding(src_emb)\n",
        "    tgt_emb = self.positional_encoding(tgt_emb)\n",
        "\n",
        "    for i, encoder in enumerate(self.transformer_encoders):\n",
        "      if i == 0:\n",
        "        enc_output_cat = encoder(src_emb, src_mask).unsqueeze(0)\n",
        "      else:\n",
        "        enc_output     = encoder(enc_output_cat[-1], src_mask).unsqueeze(0)\n",
        "        enc_output_cat = torch.cat((enc_output_cat, enc_output), dim=0)\n",
        "\n",
        "    for i, decoder in enumerate(self.transformer_decoders):\n",
        "      if i == 0:\n",
        "        dec_output = decoder(tgt_emb, enc_output_cat[i], tgt_mask)\n",
        "      else:\n",
        "        dec_output = decoder(dec_output, enc_output_cat[i], tgt_mask)\n",
        "    \n",
        "    return self.fc_out(dec_output)\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src_emb = self.src_token_embedding(src)\n",
        "    src_emb = self.positional_encoding(src_emb)\n",
        "\n",
        "    for i, encoder in enumerate(self.transformer_encoders):\n",
        "      if i == 0:\n",
        "        enc_output_cat = encoder(src_emb, src_mask).unsqueeze(0)\n",
        "      else:\n",
        "        enc_output     = encoder(enc_output_cat[-1], src_mask).unsqueeze(0)\n",
        "        enc_output_cat = torch.cat((enc_output_cat, enc_output), dim=0)\n",
        "    return enc_output_cat\n",
        "\n",
        "  def decode(self, tgt, memory, tgt_mask):\n",
        "    tgt_emb = self.tgt_token_embedding(tgt)\n",
        "    tgt_emb = self.positional_encoding(tgt_emb)\n",
        "\n",
        "    for i, decoder in enumerate(self.transformer_decoders):\n",
        "      if i == 0:\n",
        "        dec_output = decoder(tgt_emb, memory[i], tgt_mask)\n",
        "      else:\n",
        "        dec_output = decoder(dec_output, memory[i], tgt_mask)\n",
        "    return dec_output"
      ],
      "metadata": {
        "id": "brOKOckM2hMn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper parameters\n",
        "INPUT_DIM, OUTPUT_DIM = len(en_vocab), len(de_vocab)\n",
        "HIDDEN_DIM = 512\n",
        "LAYERS     = 6\n",
        "HEADS      = 8\n",
        "PF_DIM     = 2048\n",
        "DROPOUT    = 0.1"
      ],
      "metadata": {
        "id": "9LY5haZF9rEc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer_(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, LAYERS, HEADS, PF_DIM, DROPOUT)\n",
        "\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters.\\n')\n",
        "\n",
        "def initialize_weights(model):\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSjc8AHv9snL",
        "outputId": "544aeccb-dd62-43a1-9270-35726fe75a4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 55,522,638 trainable parameters.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# using adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=0.0001,\n",
        "                             betas=(0.9, 0.98),\n",
        "                             eps=1e-9) \n",
        "\n",
        "# define loss function, ignore for padding value\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=preprocessor.pad_token_id)"
      ],
      "metadata": {
        "id": "98mA191x-Osg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "sHiSiSUU9HPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, loss_fn, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    src = batch[0].T\n",
        "    tgt = batch[1].T\n",
        "    \n",
        "    optimizer.zero_grad()  # make gradients zero before backpropagation\n",
        "\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:-1,:])\n",
        "\n",
        "    output = model(src, tgt[:-1,:],\n",
        "                   src_mask, tgt_mask,\n",
        "                   src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    \n",
        "    tgt = tgt[1:, :].reshape(-1)  # ignore for target's <sos>\n",
        "    output = output.reshape(-1, output.shape[-1])\n",
        "    \n",
        "    loss = loss_fn(output, tgt)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    loss.backward()                                           # compute gradient\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # gradient clipping\n",
        "    optimizer.step()                                          # update parameters\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, loss_fn):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      src = batch[0].T\n",
        "      tgt = batch[1].T\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:-1,:])\n",
        "\n",
        "      output = model(src, tgt[:-1, :],\n",
        "                    src_mask, tgt_mask,\n",
        "                    src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "      tgt = tgt[1:, :].reshape(-1)  # ignore for target's <sos>\n",
        "      output = output.reshape(-1, output.shape[-1])\n",
        "\n",
        "      loss = loss_fn(output, tgt)\n",
        "      epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "9jouuRdD9Gw2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 10\n",
        "CLIP   = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_loss = train(model, train_loader, optimizer, loss_fn, CLIP)\n",
        "  valid_loss = evaluate(model, valid_loader, loss_fn)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'transformer_en_to_de.pt')\n",
        "\n",
        "  print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "  print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck9F7SQ09K3H",
        "outputId": "768f9048-a313-445b-b22f-11032f49882a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 0s\n",
            "\tTrain Loss: 4.655 | Train PPL: 105.135\n",
            "\tValidation Loss: 3.535 | Validation PPL: 34.294\n",
            "\n",
            "\n",
            "Epoch: 02 | Time: 1m 0s\n",
            "\tTrain Loss: 3.292 | Train PPL: 26.892\n",
            "\tValidation Loss: 2.834 | Validation PPL: 17.006\n",
            "\n",
            "\n",
            "Epoch: 03 | Time: 0m 59s\n",
            "\tTrain Loss: 2.729 | Train PPL: 15.322\n",
            "\tValidation Loss: 2.427 | Validation PPL: 11.320\n",
            "\n",
            "\n",
            "Epoch: 04 | Time: 0m 59s\n",
            "\tTrain Loss: 2.356 | Train PPL: 10.546\n",
            "\tValidation Loss: 2.143 | Validation PPL: 8.526\n",
            "\n",
            "\n",
            "Epoch: 05 | Time: 0m 59s\n",
            "\tTrain Loss: 2.083 | Train PPL: 8.029\n",
            "\tValidation Loss: 1.964 | Validation PPL: 7.129\n",
            "\n",
            "\n",
            "Epoch: 06 | Time: 1m 0s\n",
            "\tTrain Loss: 1.867 | Train PPL: 6.467\n",
            "\tValidation Loss: 1.820 | Validation PPL: 6.175\n",
            "\n",
            "\n",
            "Epoch: 07 | Time: 0m 59s\n",
            "\tTrain Loss: 1.687 | Train PPL: 5.406\n",
            "\tValidation Loss: 1.724 | Validation PPL: 5.608\n",
            "\n",
            "\n",
            "Epoch: 08 | Time: 0m 59s\n",
            "\tTrain Loss: 1.539 | Train PPL: 4.659\n",
            "\tValidation Loss: 1.660 | Validation PPL: 5.259\n",
            "\n",
            "\n",
            "Epoch: 09 | Time: 0m 59s\n",
            "\tTrain Loss: 1.412 | Train PPL: 4.105\n",
            "\tValidation Loss: 1.606 | Validation PPL: 4.984\n",
            "\n",
            "\n",
            "Epoch: 10 | Time: 0m 59s\n",
            "\tTrain Loss: 1.299 | Train PPL: 3.667\n",
            "\tValidation Loss: 1.561 | Validation PPL: 4.763\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download saved model\n",
        "from google.colab import files\n",
        "files.download('transformer_en_to_de.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dew_b5yD9MTW",
        "outputId": "37c2822a-d6e3-4ce1-afa4-de561bc58734"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_77c381a3-ca62-40af-bb28-0d65b64f0b7d\", \"transformer_en_to_de.pt\", 232399055)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing"
      ],
      "metadata": {
        "id": "a5pAsslk9TFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved model\n",
        "model.load_state_dict(torch.load('./transformer_en_to_de.pt'))\n",
        "\n",
        "# test\n",
        "test_loss = evaluate(model, test_loader, loss_fn)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qamAZz7W9Tde",
        "outputId": "39f64cd7-784a-4991-a8ad-178514d2feca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.626 | Test PPL: 5.085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# translation using my model\n",
        "def translate_sentence(sentence, preprocessor, model, max_len=50):\n",
        "  model.eval()\n",
        "  \n",
        "  # make source indices\n",
        "  src_indices = preprocessor.src_encode(sentence)\n",
        "  src_indices = torch.LongTensor(src_indices).view(-1, 1).to(DEVICE)\n",
        "  # src_indices: [seq_len, 1]\n",
        "  \n",
        "  # using encoder\n",
        "  num_tokens = src_indices.shape[0]\n",
        "  src_mask   = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE)\n",
        "  enc_src    = model.encode(src_indices, src_mask)\n",
        "\n",
        "  # using decoder, make translated target indices\n",
        "  tgt_indices = torch.ones(1, 1).fill_(preprocessor.sos_token_id).type(torch.long).to(DEVICE)\n",
        "  for i in range(max_len):\n",
        "    enc_src  = enc_src.to(DEVICE)\n",
        "    tgt_mask = (generate_square_subsequent_mask(tgt_indices.size(0)).type(torch.bool)).to(DEVICE)\n",
        "\n",
        "    output = model.decode(tgt_indices, enc_src, tgt_mask)\n",
        "    output = output.transpose(0, 1)\n",
        "    prob   = model.fc_out(output[:, -1])\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.item()\n",
        "\n",
        "    tgt_indices = torch.cat([tgt_indices,\n",
        "                             torch.ones(1, 1).type_as(src_indices.data).fill_(next_word)], dim=0)\n",
        "    if next_word == preprocessor.eos_token_id: break\n",
        "  \n",
        "  tgt_tokens = [preprocessor.tgt_id2token[id] for id in tgt_indices]\n",
        "  return tgt_tokens[1:-1]"
      ],
      "metadata": {
        "id": "nMX3fo9e9Ven"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(test_dataset):\n",
        "  if i == 10:\n",
        "    src, tgt = batch[0], batch[1]\n",
        "    break\n",
        "src = preprocessor.src_decode(src)\n",
        "tgt = preprocessor.tgt_decode(tgt)\n",
        "print('original en sentence:')\n",
        "print(f'\\t{src}')\n",
        "print('original de sentence:')\n",
        "print(f'\\t{tgt}')\n",
        "\n",
        "translation = translate_sentence(src, preprocessor, model)\n",
        "translation = ' '.join(translation)\n",
        "print('translated de sentence:')\n",
        "print(f'\\t{translation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSvAVFVK9XWT",
        "outputId": "f81864c6-5b26-4114-ab42-ce0628bf1eff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original en sentence:\n",
            "\tA mother and her young song enjoying a beautiful day outside .\n",
            "original de sentence:\n",
            "\tEine Mutter und ihr kleiner Sohn genießen einen schönen Tag im Freien .\n",
            "translated de sentence:\n",
            "\tEine Mutter und ihr junger Erwachsener genießen einen schönen Tag im Freien .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for bleu score\n",
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "TWO1i6--9Y2g",
        "outputId": "55ddbe63-b7dd-41c8-a949-ccd85147cfcf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.12.1+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.9.24)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def show_bleu(data, preprocessor, model, max_len=50):\n",
        "  trgs, pred_trgs = [], []\n",
        "  for i in range(len(data)):\n",
        "    # original target\n",
        "    src, trg = data[i][0], data[i][1]\n",
        "    src = preprocessor.src_decode(src)\n",
        "    trg = preprocessor.tgt_decode(trg).split(' ')\n",
        "    trgs.append([trg])\n",
        "\n",
        "    # predicted target\n",
        "    pred_trg = translate_sentence(src, preprocessor, model)\n",
        "    pred_trg = pred_trg[1:-1]\n",
        "    pred_trgs.append(pred_trg)\n",
        "\n",
        "    if (i + 1)%100 == 0:\n",
        "      print(f\"[{i+1}/{len(data)}]\")\n",
        "      print(f\"정답: {trg}\")\n",
        "      print(f\"예측: {pred_trg}\\n\")\n",
        "\n",
        "  bleu = bleu_score(pred_trgs, trgs, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "  print(f'Total BLEU Score = {bleu*100:.2f}')\n",
        "\n",
        "  bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "  bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 1, 0, 0])\n",
        "  bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 1, 0])\n",
        "  bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 0, 1])\n",
        "  print(f'BLEU1 score = {bleu1_score}') \n",
        "  print(f'BLEU2 score = {bleu2_score}') \n",
        "  print(f'BLEU3 score = {bleu3_score}') \n",
        "  print(f'BLEU4 score = {bleu4_score}') \n",
        "\n",
        "show_bleu(test_dataset, preprocessor, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG5Lt26g9aEZ",
        "outputId": "2a76b710-1894-46e8-fbbf-4657e4a8c799"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100/1000]\n",
            "정답: ['Ein', 'kleiner', 'Junge', 'im', 'Fußballdress', 'hält', 'die', 'Hände', '<unk>', 'Gesicht', 'und', 'weint', '.']\n",
            "예측: ['kleiner', 'Junge', 'in', 'einem', 'Fußball', ',', 'der', 'in', 'sein', '<unk>', '<unk>', '<unk>', '<unk>']\n",
            "\n",
            "[200/1000]\n",
            "정답: ['Ein', 'Mann', 'macht', 'Werbung', 'mit', 'einem', 'riesigen', 'Schild', ',', 'das', 'auf', 'sein', 'Fahrrad', 'gebunden', 'ist', '.']\n",
            "예측: ['Mann', 'mit', 'einem', 'riesigen', '<unk>', 'mit', 'einem', 'riesigen', 'Schild', 'auf', 'seinem', 'Fahrrad']\n",
            "\n",
            "[300/1000]\n",
            "정답: ['Eine', 'Gruppe', 'junger', 'Menschen', 'trinkt', '<unk>', 'in', 'einem', '<unk>', '<unk>', '.']\n",
            "예측: ['Gruppe', 'junger', 'Menschen', 'bereitet', 'in', 'einem', '<unk>', '<unk>', 'zu']\n",
            "\n",
            "[400/1000]\n",
            "정답: ['Ein', 'lächelnder', 'Junge', 'spielt', 'im', 'Laub', 'mit', 'den', 'Enten', '.']\n",
            "예측: ['lächelnder', 'kleiner', 'Junge', 'spielt', 'zwischen', 'mehreren', 'Blättern', 'im', 'Laub']\n",
            "\n",
            "[500/1000]\n",
            "정답: ['Eine', 'Frau', 'steht', 'auf', 'einem', 'grünen', 'Feld', ',', 'hält', 'einen', 'weißen', 'Hund', 'und', 'zeigt', 'auf', 'einen', 'braunen', 'Hund', '.']\n",
            "예측: ['Frau', 'steht', 'in', 'einem', 'grünen', 'Feld', 'und', 'zeigt', 'auf', 'ein', 'weißes', 'Hund', ',', 'das', 'auf', 'ein', 'brauner', 'Hund', 'zeigt']\n",
            "\n",
            "[600/1000]\n",
            "정답: ['Ein', 'Typ', 'in', 'einem', 'gelben', 'Outfit', 'steht', 'hinter', 'dem', 'Mikrofon', 'in', 'einem', 'Zelt', '.']\n",
            "예측: ['Typ', 'in', 'gelber', 'Kleidung', 'steht', 'hinter', 'einem', 'Zelt', 'in', 'einem', 'Zelt']\n",
            "\n",
            "[700/1000]\n",
            "정답: ['Zwei', 'Motocrossfahrer', 'in', 'voller', 'Schutzkleidung', ',', 'einer', 'von', 'ihnen', 'ist', 'nach', 'einem', 'Sprung', 'in', 'der', 'Luft', ',', 'der', 'andere', 'blickt', 'auf', 'sein', 'Motorrad', 'hinunter', '.']\n",
            "예측: ['<unk>', 'Radfahrer', 'mit', 'Schutzausrüstung', 'springen', 'nach', 'einem', 'Sprung', 'in', 'der', 'Luft', ',', 'während', 'ein', 'anderer', 'Mann', 'auf', 'seinem', 'Motorrad', 'blickt']\n",
            "\n",
            "[800/1000]\n",
            "정답: ['Ein', 'Mann', 'sitzt', 'auf', 'einer', 'Plattform', 'mit', 'Rädern', 'und', 'wird', 'an', 'einem', 'Loch', '<unk>', '.']\n",
            "예측: ['Mann', 'sitzt', 'auf', 'einer', 'Plattform', 'mit', 'Rädern', 'von', 'Rädern', 'von', 'einem', '<unk>']\n",
            "\n",
            "[900/1000]\n",
            "정답: ['Eine', 'Person', 'in', 'Blau', 'wirft', 'auf', 'einer', 'Bowlingbahn', 'als', '<unk>', 'ihre', 'Kugel', '.']\n",
            "예측: ['Person', 'in', 'Blau', 'ist', '<unk>', ',', 'die', 'sich', 'auf', 'einer', 'Bowlingbahn', 'zu', 'werfen', ',', 'wirft', ',', 'das', 'sie', 'auf', 'eine', 'Bowlingbahn', 'wirft']\n",
            "\n",
            "[1000/1000]\n",
            "정답: ['Ein', 'Mädchen', 'an', 'einer', 'Küste', 'mit', 'einem', 'Berg', 'im', 'Hintergrund', '.']\n",
            "예측: ['Mädchen', 'am', 'Strand', 'mit', 'einem', 'Berg', 'in', 'der', 'Ferne']\n",
            "\n",
            "Total BLEU Score = 26.34\n",
            "BLEU1 score = 0.5248907173460342\n",
            "BLEU2 score = 0.32435306044609513\n",
            "BLEU3 score = 0.21021864138728522\n",
            "BLEU4 score = 0.13448899873984502\n"
          ]
        }
      ]
    }
  ]
}